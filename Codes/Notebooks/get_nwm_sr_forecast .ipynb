{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d6d86b",
   "metadata": {},
   "source": [
    "# National Water Model: Short Range Forecast Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825919b7",
   "metadata": {},
   "source": [
    "**Author(s):** \n",
    "\n",
    "<ul style=\"line-height:1.5;\">\n",
    "<li>Nana Oye Djan <a href=\"mailto:ndjan@andrew.cmu.edu\">(ndjan@andrew.cmu.edu)</a></li>\n",
    "<li>Kayode Adebayo <a href=\"mailto:kayode.adebayo@jacks.sdstate.edu\">(kayode.adebayo@jacks.sdstate.edu)</a></li>\n",
    "<li>Saide Zand <a href=\"mailto:szand@crimson.ua.edu\">(szand@crimson.ua.edu)</a></li>\n",
    "</ul>\n",
    "\n",
    "**Last Updated:** \n",
    "17th July 2025\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This notebook provides code to retrieve NOAA National Water Model Short Range data from Amazon Web Services (AWS) in netcdf format for Travis County, Texas.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook downloads an ensmeble of 10 forecasts for each forecast time, from current time to 10 hours ahead, run at different initialization times. It takes a csv file of COMIDs, a layer in a geodatase with information on the geometries of the reaches (COMIDs) then retrieves data from AWS for Travis County, merges with the geodatabase layer, and saves them as shapefiles.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This notebook uses data developed and published by NOAA on Amazon Web Services (AWS) as described in detail in <a href=\"https://registry.opendata.aws/noaa-nwm-pds/\">(this registry)</a></li> of open data entry. The National Water Model (NWM) is a water resources model that simulates and forecasts water budget variables, including snowpack, evapotranspiration, soil moisture and streamflow, over the entire continental United States (CONUS). It is operated by NOAA’s Office of Water Prediction. This bucket contains a four-week rollover of the Short Range Forecast model output and the corresponding forcing data for the model. The model is forced with meteorological data from the High Resolution Rapid Refresh (HRRR) and the Rapid Refresh (RAP) models. The Short Range Forecast configuration cycles hourly and produces hourly deterministic forecasts of streamflow and hydrologic states out to 18 hours. It also uses information on Travis County flowlines provided <a href=\"http://www.hydroshare.org/resource/c95e654312204ce0b4d8e31e71cd4354\">(here)</a></li>\n",
    "\n",
    "**Software Requirements:**\n",
    "\n",
    "This notebook uses Python v3.10.14 and requires the following specific Python libraries: \n",
    "\n",
    "> xarray: 2025.4.0     \n",
    "   geopandas: 0.14.4  \n",
    "   pandas: 2.3.0 \n",
    "   s3fs: 2025.5.1  \n",
    "   fsspec: 2025.5.1    \n",
    "   ipython: 8.37.0 \\\n",
    "   os: Python 3.10.14 (stdlib) \\\n",
    "   datetime / timedelta: Python 3.10.14 (stdlib) \\\n",
    "   re: Python 3.10.14 (stdlib)   \n",
    "\n",
    "**Disclosure**\n",
    "The code contained in this notebook was partially created and revised by ChatGPT, an AI language model developed by OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1b051",
   "metadata": {},
   "source": [
    "### 1. Import Python Libraries Needed to Run this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b971054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "from IPython.display import display\n",
    "import fsspec\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da220eb",
   "metadata": {},
   "source": [
    "### 2. Read In Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc88903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary files/Define necessary parameters\n",
    "#Travis County COMIDS\n",
    "comid_csv = '/Travis_Feature_IDs.csv'\n",
    "\n",
    "#Geopackage and layer with Travis County Flowlines\n",
    "flowlines_gpkg = '/Theme4DataRevised/Theme4Data.gdb'\n",
    "flowline_layer = 'P2FFlowlines'\n",
    "\n",
    "#Output folder for shapefile\n",
    "out_dir = \"path/to/output/dir\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#S3 bucket info\n",
    "s3_bucket = 'noaa-nwm-pds'\n",
    "forecast_path = 'short_range'\n",
    "variable = 'streamflow'\n",
    "filename = 'channel_rt'\n",
    "\n",
    "#Parameters\n",
    "valid_time = datetime.utcnow()\n",
    "max_lead_hours = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0901b",
   "metadata": {},
   "source": [
    "### 3. Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a338a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "# URL builder\n",
    "def construct_s3_url(cycle_dt: datetime, lead_hr: int) -> str:\n",
    "    \"\"\"\n",
    "    Build the S3 URL for:\n",
    "      nwm.t{HH}z.short_range.{filename}.f{LLL}.conus.nc\n",
    "    \"\"\"\n",
    "    date_str = cycle_dt.strftime(\"%Y%m%d\")      # e.g. '20250708'\n",
    "    cycle_hr = f\"{cycle_dt.hour:02d}\"           # e.g. '16'\n",
    "    lead_str = f\"{lead_hr:03d}\"                 # e.g. '005'\n",
    "    fname    = (\n",
    "        f\"nwm.t{cycle_hr}z.short_range.\"\n",
    "        f\"{filename}.f{lead_str}.conus.nc\"\n",
    "    )\n",
    "    return f\"s3://{s3_bucket}/nwm.{date_str}/{forecast_path}/{fname}\"\n",
    "\n",
    "#Function to calculate most likely \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def collapse_with_runs(df):\n",
    "    df = df.copy()\n",
    "    # rename to the function’s expected names\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"forecast_r\":\"forecast_run\",  # already correct\n",
    "            \"forecast_t\":\"forecast_time\"   # already correct\n",
    "        }\n",
    "    )\n",
    "    # parse dates\n",
    "    df[\"forecast_r_dt\"] = pd.to_datetime(df[\"forecast_run\"])\n",
    "    df[\"forecast_t_dt\"] = pd.to_datetime(df[\"forecast_time\"])\n",
    "    \n",
    "    records = []\n",
    "    for fid, grp in df.groupby(\"feature_id\", sort=False):\n",
    "        valid = grp[grp[\"forecast_r_dt\"] <= grp[\"forecast_t_dt\"]].copy()\n",
    "        valid[\"lead_sec\"] = (valid[\"forecast_t_dt\"] - valid[\"forecast_r_dt\"]) \\\n",
    "                              .dt.total_seconds()\n",
    "        top2 = valid.nlargest(2, \"lead_sec\")\n",
    "\n",
    "        if not top2.empty:\n",
    "            ml_dis = top2[\"streamflow\"].max()\n",
    "            ml_row = top2.loc[top2[\"streamflow\"].idxmax()]\n",
    "            ml_r   = ml_row[\"forecast_run\"]\n",
    "            ml_t   = ml_row[\"forecast_time\"]\n",
    "        else:\n",
    "            ml_dis, ml_r, ml_t = None, None, None\n",
    "\n",
    "        wc_dis = grp[\"streamflow\"].max()\n",
    "        wc_row = grp.loc[grp[\"streamflow\"].idxmax()]\n",
    "        wc_r   = wc_row[\"forecast_run\"]\n",
    "        wc_t   = wc_row[\"forecast_time\"]\n",
    "\n",
    "        records.append({\n",
    "            \"feature_id\": fid,\n",
    "            \"ml_dis\":     ml_dis,\n",
    "            \"ml_run\":       ml_r,\n",
    "            \"ml_time\":       ml_t,\n",
    "            \"wc_dis\":     wc_dis,\n",
    "            \"wc_run\":       wc_r,\n",
    "            \"wc_time\":       wc_t\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02805e",
   "metadata": {},
   "source": [
    "### 4. Load COMIDs and Flowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e2413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 910 flowlines matching COMIDs.\n"
     ]
    }
   ],
   "source": [
    "# Read list of COMIDs\n",
    "comids = pd.read_csv(comid_csv, dtype={\"IDs\": str})[\"IDs\"].tolist()\n",
    "\n",
    "# Read flowlines geopackage and filter\n",
    "flows = gpd.read_file(flowlines_gpkg, layer=flowline_layer)\n",
    "flows = flows.rename(columns={\"ID_int\":\"feature_id\"})\n",
    "flows[\"feature_id\"] = flows[\"feature_id\"].astype(str)\n",
    "\n",
    "print(f\"Loaded {len(comids)} flowlines matching COMIDs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c67374",
   "metadata": {},
   "source": [
    "### 5. Fetch forecasts 10 hours ahead from current time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187ed35",
   "metadata": {},
   "source": [
    "**This code also gets 10 hours of forecasts from previous runs of the NWM to create an ensemble of 10 forecasted streamflows for each forecast time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a13d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using folder noaa-nwm-pds/nwm.20250717/short_range/ with cycles: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]Z\n",
      "run=16Z lead=01h → 910/910 COMIDs present\n",
      "run=16Z lead=02h → 910/910 COMIDs present\n",
      "run=16Z lead=03h → 910/910 COMIDs present\n",
      "run=16Z lead=04h → 910/910 COMIDs present\n",
      "run=16Z lead=05h → 910/910 COMIDs present\n",
      "run=16Z lead=06h → 910/910 COMIDs present\n",
      "run=16Z lead=07h → 910/910 COMIDs present\n",
      "run=16Z lead=08h → 910/910 COMIDs present\n",
      "run=16Z lead=09h → 910/910 COMIDs present\n",
      "run=16Z lead=10h → 910/910 COMIDs present\n",
      "✔ Using t16Z as latest run:\n",
      "✔ Ensemble of all forecasts for each valid_time:\n"
     ]
    }
   ],
   "source": [
    "# initialize filesystem\n",
    "fs = s3fs.S3FileSystem(anon=True, client_kwargs={\"region_name\":\"us-east-1\"})\n",
    "today = datetime.utcnow().date()\n",
    "#find most recent folder (up to 3 days back) with channel_rt files\n",
    "valid_prefix = None\n",
    "published_runs = set()\n",
    "for delta in range(4):\n",
    "    dt     = today - timedelta(days=delta)\n",
    "    prefix = f\"{s3_bucket}/nwm.{dt:%Y%m%d}/short_range/\"\n",
    "    try:\n",
    "        all_files = fs.find(prefix)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    runs = {\n",
    "        int(m.group(1))\n",
    "        for f in all_files\n",
    "        if f.endswith(\".conus.nc\")\n",
    "        and \".channel_rt.\" in f\n",
    "        and (m := re.search(r\"nwm\\.t(\\d{2})z\", os.path.basename(f)))\n",
    "    }\n",
    "    if runs:\n",
    "        valid_prefix   = prefix\n",
    "        published_runs = runs\n",
    "        break\n",
    "\n",
    "if not valid_prefix:\n",
    "    raise RuntimeError(\"No short_range/channel_rt folder in last 4 days\")\n",
    "\n",
    "# clear any cached listings so we see newly uploaded runs\n",
    "fs.invalidate_cache(valid_prefix)\n",
    "\n",
    "# rebuild channel_files with bare keys\n",
    "channel_files = [\n",
    "    f for f in fs.find(valid_prefix)\n",
    "    if f.endswith(\".conus.nc\") and \".channel_rt.\" in f\n",
    "]\n",
    "\n",
    "print(f\"Using folder {valid_prefix} with cycles: {sorted(published_runs)}Z\")\n",
    "\n",
    "# build a lookup of (run_hr, lead_hr) → bare S3 key\n",
    "run_lead_map = {}\n",
    "for path in channel_files:\n",
    "    fn     = os.path.basename(path)\n",
    "    m_run  = re.search(r\"nwm\\.t(\\d{2})z\", fn)\n",
    "    m_lead = re.search(r\"\\.f(\\d{3})\\.\", fn)\n",
    "    if m_run and m_lead:\n",
    "        run_hr, lead_hr = int(m_run.group(1)), int(m_lead.group(1))\n",
    "        if 1 <= lead_hr <= max_lead_hours:\n",
    "            run_lead_map[(run_hr, lead_hr)] = path  # <— no \"s3://\"\n",
    "\n",
    "# parse the prefix date for full datetimes\n",
    "date_str  = valid_prefix.split(\"/\")[1].split(\".\")[1]  # e.g. \"20250708\"\n",
    "prefix_dt = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "\n",
    "# Latest‐cycle forecasts (tXXZ → f001–f010)\n",
    "latest_run_hr = max(run for run, _ in run_lead_map)\n",
    "latest_run_dt = prefix_dt + timedelta(hours=latest_run_hr)\n",
    "\n",
    "comids_int       = [int(c) for c in comids]\n",
    "records_current  = []\n",
    "chosen_run_hr    = None\n",
    "\n",
    "for run_hr in sorted(published_runs, reverse=True):\n",
    "    run_dt = prefix_dt + timedelta(hours=run_hr)\n",
    "    temp   = []\n",
    "\n",
    "    for lead_hr in range(1, max_lead_hours+1):\n",
    "        key = run_lead_map.get((run_hr, lead_hr))\n",
    "        if not key:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with fs.open(key, \"rb\") as fobj:\n",
    "                ds = xr.open_dataset(fobj, engine=\"h5netcdf\")\n",
    "\n",
    "                # integer‐based intersection\n",
    "                ds_ids      = ds[\"feature_id\"].values.astype(int)\n",
    "                present_int = [cid for cid in comids_int if cid in ds_ids]\n",
    "                print(f\"run={run_hr:02d}Z lead={lead_hr:02d}h → \"\n",
    "                      f\"{len(present_int)}/{len(comids_int)} COMIDs present\")\n",
    "\n",
    "                if not present_int:\n",
    "                    continue\n",
    "\n",
    "                da = ds[variable].sel(feature_id=present_int).load()\n",
    "        except (FileNotFoundError, OSError):\n",
    "            continue\n",
    "\n",
    "        # build the DataFrame for this slice\n",
    "        df = da.to_dataframe().reset_index()\n",
    "        df[\"feature_id\"]   = df[\"feature_id\"].astype(str)\n",
    "        df = df.set_index(\"feature_id\").reindex(comids).reset_index()\n",
    "        df[\"valid_time\"]   = run_dt + timedelta(hours=lead_hr)\n",
    "        df[\"forecast_run\"] = run_dt\n",
    "        df[\"lead_hour\"]    = lead_hr\n",
    "        temp.append(df)\n",
    "\n",
    "    if temp:\n",
    "        chosen_run_hr   = run_hr\n",
    "        records_current = temp\n",
    "        break\n",
    "\n",
    "if not records_current:\n",
    "    raise RuntimeError(\"No 1–10h forecasts found for any recent run\")\n",
    "\n",
    "latest_run_hr = chosen_run_hr\n",
    "latest_run_dt = prefix_dt + timedelta(hours=latest_run_hr)\n",
    "df_current    = pd.concat(records_current, ignore_index=True)\n",
    "\n",
    "# … after building df_current …\n",
    "print(f\"✔ Using t{latest_run_hr:02d}Z as latest run:\")\n",
    "\n",
    "# pull ensemble for each valid_time \n",
    "records_ensemble = []\n",
    "valid_times      = [latest_run_dt + timedelta(hours=h) for h in range(1, max_lead_hours+1)]\n",
    "\n",
    "for valid_time in valid_times:\n",
    "    for lead_hr in range(1, max_lead_hours+1):\n",
    "        run_dt = valid_time - timedelta(hours=lead_hr)\n",
    "        key    = run_lead_map.get((run_dt.hour, lead_hr))\n",
    "        if not key:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with fs.open(key, \"rb\") as fobj:\n",
    "                ds = xr.open_dataset(fobj, engine=\"h5netcdf\")\n",
    "\n",
    "                ds_ids      = ds[\"feature_id\"].values.astype(int)\n",
    "                present_int = [cid for cid in comids_int if cid in ds_ids]\n",
    "                if not present_int:\n",
    "                    continue\n",
    "\n",
    "                da = ds[variable].sel(feature_id=present_int).load()\n",
    "        except (FileNotFoundError, OSError):\n",
    "            continue\n",
    "\n",
    "        df = da.to_dataframe().reset_index()\n",
    "        df[\"feature_id\"]   = df[\"feature_id\"].astype(str)\n",
    "        df = df.set_index(\"feature_id\").reindex(comids).reset_index()\n",
    "        df[\"valid_time\"]   = valid_time\n",
    "        df[\"forecast_run\"] = run_dt\n",
    "        df[\"lead_hour\"]    = lead_hr\n",
    "        records_ensemble.append(df)\n",
    "\n",
    "df_ensemble = pd.concat(records_ensemble, ignore_index=True)\n",
    "\n",
    "print(\"✔ Ensemble of all forecasts for each valid_time:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe878be0",
   "metadata": {},
   "source": [
    "### 6. Save outputs as shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a2814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-concat the full ensemble\n",
    "df_all = pd.concat(records_ensemble, ignore_index=True)\n",
    "# Rename valid_time → forecast_time\n",
    "df_all = df_all.rename(columns={\"valid_time\": \"forecast_time\"})\n",
    "\n",
    "# Make sure your datetime columns are Timestamps\n",
    "df_all['forecast_run'] = pd.to_datetime(df_all['forecast_run'])\n",
    "df_all['forecast_time']   = pd.to_datetime(df_all['forecast_time'])\n",
    "\n",
    "# compute “next hour” off your chosen latest run\n",
    "latest_run = df_current[\"forecast_run\"].max()\n",
    "next_hour = latest_run + timedelta(hours=1)\n",
    "\n",
    "df1_multi = df_all.query(\n",
    "    \"forecast_time == @next_hour and lead_hour >= 1\"\n",
    ")[[\"feature_id\",\"forecast_time\",\"forecast_run\", variable]]\n",
    "\n",
    "# rename and merge as before\n",
    "df1_multi = df1_multi.rename(columns={variable:\"streamflow\"})\n",
    "df1_multi[\"streamflow\"] = df1_multi[\"streamflow\"].fillna(0)\n",
    "\n",
    "# Calculate wc_dis and ml_dis \n",
    "collapse_1h = collapse_with_runs(df1_multi)\n",
    "keep = [\n",
    "    \"LakeID\", \"HydroID\", \"From_Node\", \"To_Node\",\n",
    "    \"NextDownID\", \"feature_id\", \"order_\", \"areasqkm\", \"geometry\"\n",
    "]\n",
    "\n",
    "gdf1_multi = (flows[keep].merge(\n",
    "    collapse_1h, on=\"feature_id\", how=\"left\"\n",
    "))\n",
    "\n",
    "for col in [\"ml_time\",\"wc_time\",\"ml_run\",\"wc_run\"]:\n",
    "    # if they’re pandas Timestamps, use strftime for consistent formatting\n",
    "    if pd.api.types.is_datetime64_any_dtype(gdf1_multi[col]):\n",
    "        gdf1_multi[col] = gdf1_multi[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        gdf1_multi[col] = gdf1_multi[col].astype(str)\n",
    "\n",
    "gdf1_multi.to_file(\"sr_nwm_tc_1hour_allruns.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92481ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2-hour ahead (all runs) → sr_nwm_tc_2hour_allruns.shp\n"
     ]
    }
   ],
   "source": [
    "#For 2 hour shapefile\n",
    "# compute the target datetime for “2 hours ahead of the latest run”\n",
    "latest_run = df_current[\"forecast_run\"].max()\n",
    "target_time = latest_run + timedelta(hours=2)\n",
    "\n",
    "# pull every forecast (any run) that lands on that exact datetime\n",
    "df2_multi = df_all.query(\"forecast_time == @target_time\")[\n",
    "    [\"feature_id\",\"forecast_time\",\"forecast_run\", variable]\n",
    "].rename(columns={variable: \"streamflow\"})\n",
    "df2_multi[\"streamflow\"] = df2_multi[\"streamflow\"].fillna(0)\n",
    "\n",
    "collapse_2h = collapse_with_runs(df2_multi)\n",
    "\n",
    "# merge on your flowlines (keeping exactly the columns you want)\n",
    "keep = [\n",
    "    \"LakeID\", \"HydroID\", \"From_Node\", \"To_Node\",\n",
    "    \"NextDownID\", \"feature_id\", \"order_\", \"areasqkm\", \"geometry\"\n",
    "]\n",
    "gdf2_multi = (\n",
    "    flows[keep]\n",
    "      .merge(collapse_2h, on=\"feature_id\", how=\"left\")\n",
    ")\n",
    "for col in [\"ml_time\",\"wc_time\",\"ml_run\",\"wc_run\"]:\n",
    "    # if they’re pandas Timestamps, use strftime for consistent formatting\n",
    "    if pd.api.types.is_datetime64_any_dtype(gdf2_multi[col]):\n",
    "        gdf2_multi[col] = gdf2_multi[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        gdf2_multi[col] = gdf2_multi[col].astype(str)\n",
    "\n",
    "# write out to shapefile\n",
    "gdf2_multi.to_file(\"sr_nwm_tc_2hour_allruns.shp\")\n",
    "print(\"Wrote 2-hour ahead (all runs) → sr_nwm_tc_2hour_allruns.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8aaa152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote max‐10h ensemble → sr_nwm_tc_max10hr.shp\n"
     ]
    }
   ],
   "source": [
    "#Max 10 hour discharge\n",
    "#get latest initialization time\n",
    "latest_run = df_current[\"forecast_run\"].max()\n",
    "\n",
    "# compute the per‐reach max across the full 1–10 h ensemble\n",
    "df10 = (\n",
    "    df_all\n",
    "      # df_all has: feature_id, forecast_run, forecast_time, lead_hour, variable\n",
    "      .groupby(\"feature_id\")[variable]\n",
    "      .max()\n",
    "      .reset_index()\n",
    "      .rename(columns={variable: \"streamflow\"})\n",
    ")\n",
    "\n",
    "# add the datetime (latest run) as a string\n",
    "df10[\"datetime\"] = latest_run.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#merge onto your flowlines, keeping exactly the attributes you want:\n",
    "gdf10 = (\n",
    "    flows[keep]\n",
    "      .merge(df10, on=\"feature_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "#fill any missing reaches with zero\n",
    "gdf10[\"streamflow\"] = gdf10[\"streamflow\"].fillna(0)\n",
    "\n",
    "# write out your shapefile\n",
    "gdf10.to_file(\"sr_nwm_tc_max10hr.shp\")\n",
    "print(\"Wrote max‐10h ensemble → sr_nwm_tc_max10hr.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
