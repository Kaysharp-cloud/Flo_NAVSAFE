{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ef5546-ab27-484a-95b5-2511be728e28",
   "metadata": {},
   "source": [
    "# FIM Generation using Arcgis Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df34b09-bf90-4012-b860-57fd0314c16d",
   "metadata": {},
   "source": [
    "**Author(s):** \n",
    "\n",
    "<ul style=\"line-height:1.5;\">\n",
    "<li>Nana Oye Djan <a href=\"mailto:ndjan@andrew.cmu.edu\">(ndjan@andrew.cmu.edu)</a></li>\n",
    "<li>Kayode Adebayo <a href=\"mailto:kayode.adebayo@jacks.sdstate.edu\">(kayode.adebayo@jacks.sdstate.edu)</a></li>\n",
    "<li>Saide Zand <a href=\"mailto:szand@crimson.ua.edu\">(szand@crimson.ua.edu)</a></li>\n",
    "</ul>\n",
    "\n",
    "**Last Updated:** \n",
    "17th July 2025\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This notebook provides code to translate shapefiles either from our get_nwm_sr_forecast notebook or get_nwm_ret_incidents into HAND-FIMs.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook produces HAND-FIMs using shapefiles of discharge values for each reach in Travis County and Synthetic Rating Curves (SRC) using the method outlined in Dean Djokic's resource found <a href=\"https://www.hydroshare.org/resource/23aa7866ab614687811bb70ffb13fcfe/\">(here)</a></li>.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This notebook uses the rating curves found in the geodatabase in <a href=\"https://www.hydroshare.org/resource/23aa7866ab614687811bb70ffb13fcfe/\">(this ArcGIS project)</a></li>. It also uses shapefiles generated by our get_nwm_sr_forecast notebook or get_nwm_ret_incidents notebooks.\n",
    "\n",
    "**Software Requirements:**\n",
    "\n",
    "This notebook is run in ArcGIS Pro and requires the following packages\n",
    "> arcpy \\\n",
    "    pandas \\\n",
    "    os \\\n",
    "    uuid \\\n",
    "    traceback \\\n",
    "    datetime \\\n",
    "    collections \\\n",
    "    dateutil.parser \\\n",
    "It also requires the ArcHydro toolbox for the Q to H interpolation using the rating curves.\n",
    "\n",
    "**Disclosure**\n",
    "The code contained in this notebook was partially created and revised by ChatGPT, an AI language model developed by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5d704-34c7-4763-a15a-aba1cc3847f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import traceback\n",
    "import datetime as dt\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "from dateutil.parser import parse as parse_dt\n",
    "\n",
    "# === Set your environment ===\n",
    "arcpy.env.workspace = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Theme4DataRevised_testing\\Theme4DataRevised_testing\"\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Auxiliary data needed\n",
    "#rating_curve_table = r\"C:\\Mac\\Home\\Documents\\ArcGIS\\Projects\\Theme4DataRevised\\Theme4Data.gdb\\RatingCurves\"\n",
    "rating_curve_table = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Theme4DataRevised_testing\\Theme4DataRevised_testing\\Theme4Data.gdb\\RatingCurves\"\n",
    "rating_curve= rating_curve_table \n",
    "#flood_stack = r\"C:\\Mac\\Home\\Documents\\ArcGIS\\Projects\\Theme4DataRevised\\Theme4Data.gdb\\TravisFIM\"\n",
    "flood_stack = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Theme4DataRevised_testing\\Theme4DataRevised_testing\\Theme4Data.gdb\\TravisFIM\"\n",
    "output_gdb = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Theme4DataRevised_testing\\Theme4DataRevised_testing\\Theme4Data.gdb\"\n",
    "\n",
    "# Shapefile with flows\n",
    "shp_file = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\shape_nana\\sr_nwm_tc_max10hr.shp\"\n",
    "\n",
    "### For our 3 FIMs\n",
    "\n",
    "folder = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\discharge_shapefiles_2\"\n",
    "\n",
    "def safe_add_field(table, field_name, field_type):\n",
    "    \"\"\"Add field only if it doesn't already exist.\"\"\"\n",
    "    if field_name.lower() not in [f.name.lower() for f in arcpy.ListFields(table)]:\n",
    "        arcpy.AddField_management(table, field_name, field_type)\n",
    "\n",
    "def process_fim_flow(in_fc, flow_field, out_name):\n",
    "    # clear any leftover locks\n",
    "    arcpy.ClearWorkspaceCache_management()\n",
    "\n",
    "    # 1) Bring your source in as a layer (no locks on the file itself)\n",
    "    fc_lyr = arcpy.MakeFeatureLayer_management(in_fc, \"fc_tmp\").getOutput(0)\n",
    "\n",
    "    # 2) add & calculate Q_cfs on that layer\n",
    "    safe_add_field(fc_lyr, \"Q_cfs\", \"DOUBLE\")\n",
    "    arcpy.CalculateField_management(\n",
    "        fc_lyr, \"Q_cfs\", f\"!{flow_field}! * 1\", \"PYTHON3\"\n",
    "    )\n",
    "\n",
    "    # 3) copy the layer into scratch/in_memory to work on it\n",
    "    scratch = arcpy.env.scratchGDB\n",
    "    working = arcpy.CopyFeatures_management(\n",
    "        fc_lyr, os.path.join(scratch, f\"{out_name}_wrk\")\n",
    "    ).getOutput(0)\n",
    "\n",
    "    # 4) delete the temp layer immediately\n",
    "    arcpy.Delete_management(fc_lyr)\n",
    "    \n",
    "    # 2) interpolate Q → H\n",
    "    arcpy.ImportToolbox(\n",
    "        r\"C:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcToolbox\\Toolboxes\\Arc_Hydro_Tools_Pro.tbx\",\n",
    "        \"hydro\"\n",
    "    )\n",
    "    arcpy.interpolatefromlookuptablewithfieldmappings_hydro(\n",
    "        working, rating_curve,\n",
    "        \"Q to H\", \"Q_cfs:discharge_\",\"H:stage_m\",\n",
    "        \"HydroID\",\"HydroID\"\n",
    "    )\n",
    "    # 3) compute HIndex only where H is non-null\n",
    "    lyr = arcpy.MakeFeatureLayer_management(working, \"hasH\", \"H IS NOT NULL\").getOutput(0)\n",
    "    arcpy.CalculateField_management(\n",
    "        lyr, \"HIndex\",\n",
    "        \"int(round(!H! *3.28) + 1)\", \"PYTHON3\"\n",
    "    )\n",
    "    arcpy.Delete_management(lyr)\n",
    "\n",
    "    # 4) spatial join to flood polygons\n",
    "    out_fc = os.path.join(output_gdb, out_name)\n",
    "    if arcpy.Exists(out_fc):\n",
    "        arcpy.Delete_management(out_fc)\n",
    "\n",
    "    flood_lyr  = arcpy.MakeFeatureLayer_management(flood_stack, \"flood_tmp\").getOutput(0)\n",
    "    stream_lyr = arcpy.MakeFeatureLayer_management(working,     \"strm_tmp\").getOutput(0)\n",
    "    \n",
    "    # cast join keys to LONG\n",
    "    for fld in (\"HydroID\", \"HIndex\"):\n",
    "        field_type = \"LONG\" if fld == \"HydroID\" else \"DOUBLE\"\n",
    "        safe_add_field(stream_lyr, fld + \"_num\", field_type)\n",
    "        arcpy.CalculateField_management(\n",
    "            stream_lyr, fld + \"_num\", f\"!{fld}!\", \"PYTHON3\"\n",
    "    )\n",
    "\n",
    "\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        flood_lyr, stream_lyr, out_fc,\n",
    "        \"JOIN_ONE_TO_MANY\", \"KEEP_COMMON\",\n",
    "        match_fields=[[\"HydroID_num\",\"HydroID\"], [\"HIndex_num\",\"HIndex\"]]\n",
    "    )\n",
    "    \n",
    "    # cleanup\n",
    "    for tmp in (flood_lyr, stream_lyr, working):\n",
    "        arcpy.Delete_management(tmp)\n",
    "\n",
    "    arcpy.Delete_management(working)\n",
    "\n",
    "    print(f\"✅  {out_name} written to {output_gdb}\")\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Main driver: loop the folder\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace     = folder\n",
    "\n",
    "\n",
    "\n",
    "lwc =r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Datasets\\lowwatercrossing_clean\\clean_LWC.shp\"\n",
    "\n",
    "def classify_lwc(fim_fc, output_name):\n",
    "    # Path to original LWC\n",
    "    lwc_input = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Datasets\\lowwatercrossing_clean\\clean_LWC.shp\"\n",
    "    \n",
    "    # Output path for classified LWC\n",
    "    lwc_out = os.path.join(output_gdb, f\"{output_name}_LWC\")\n",
    "\n",
    "    # Copy the LWC shapefile into the output geodatabase\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(lwc_input, output_gdb, f\"{output_name}_LWC\")\n",
    "\n",
    "    # Remove \"Condition\" if it already exists\n",
    "    fields = [f.name for f in arcpy.ListFields(lwc_out)]\n",
    "    if \"Condition\" in fields:\n",
    "        arcpy.DeleteField_management(lwc_out, \"Condition\")\n",
    "\n",
    "    # Add new Condition field\n",
    "    arcpy.AddField_management(lwc_out, \"Condition\", \"TEXT\", field_length=10)\n",
    "\n",
    "    # Make layer from LWC and FIM\n",
    "    fim_lyr = arcpy.MakeFeatureLayer_management(fim_fc, \"fim_lyr\").getOutput(0)\n",
    "    lwc_lyr = arcpy.MakeFeatureLayer_management(lwc_out, \"lwc_lyr\").getOutput(0)\n",
    "\n",
    "    # Select only LWC points that intersect the FIM\n",
    "    arcpy.SelectLayerByLocation_management(\n",
    "        lwc_lyr, \"INTERSECT\", fim_lyr, selection_type=\"NEW_SELECTION\"\n",
    "    )\n",
    "\n",
    "    # Set selected LWC points to \"Flooded\"\n",
    "    arcpy.CalculateField_management(lwc_lyr, \"Condition\", '\"Flooded\"', \"PYTHON3\")\n",
    "\n",
    "    # Switch selection and set others to \"Safe\"\n",
    "    arcpy.SelectLayerByAttribute_management(lwc_lyr, \"SWITCH_SELECTION\")\n",
    "    arcpy.CalculateField_management(lwc_lyr, \"Condition\", '\"Safe\"', \"PYTHON3\")\n",
    "\n",
    "    # Cleanup\n",
    "    arcpy.Delete_management(fim_lyr)\n",
    "    arcpy.Delete_management(lwc_lyr)\n",
    "\n",
    "    print(f\"✅  {output_name}_LWC written to {output_gdb}\")\n",
    "\n",
    "\n",
    "def classify_address_points(fim_fc, output_name):\n",
    "    # Path to original Address Points shapefile\n",
    "    address_input = r\"C:\\Users\\kbadebayo\\Documents\\ArcGIS\\Projects\\Datasets\\addresspoint\\addresspoint.shp\"\n",
    "    \n",
    "    # Output path for classified Address Points\n",
    "    address_out = os.path.join(output_gdb, f\"{output_name}_ADDR\")\n",
    "\n",
    "    # Copy to output geodatabase\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(address_input, output_gdb, f\"{output_name}_ADDR\")\n",
    "\n",
    "    # Remove \"Condition\" if it exists\n",
    "    fields = [f.name for f in arcpy.ListFields(address_out)]\n",
    "    if \"Condition\" in fields:\n",
    "        arcpy.DeleteField_management(address_out, \"Condition\")\n",
    "\n",
    "    # Add new \"Condition\" field\n",
    "    arcpy.AddField_management(address_out, \"Condition\", \"TEXT\", field_length=10)\n",
    "\n",
    "    # Make layers\n",
    "    fim_lyr = arcpy.MakeFeatureLayer_management(fim_fc, \"fim_lyr_addr\").getOutput(0)\n",
    "    addr_lyr = arcpy.MakeFeatureLayer_management(address_out, \"addr_lyr\").getOutput(0)\n",
    "\n",
    "    # Select points that intersect FIM\n",
    "    arcpy.SelectLayerByLocation_management(\n",
    "        addr_lyr, \"INTERSECT\", fim_lyr, selection_type=\"NEW_SELECTION\"\n",
    "    )\n",
    "    arcpy.CalculateField_management(addr_lyr, \"Condition\", '\"Flooded\"', \"PYTHON3\")\n",
    "\n",
    "    # Select the safe points\n",
    "    arcpy.SelectLayerByAttribute_management(addr_lyr, \"SWITCH_SELECTION\")\n",
    "    arcpy.CalculateField_management(addr_lyr, \"Condition\", '\"Safe\"', \"PYTHON3\")\n",
    "\n",
    "    # Cleanup\n",
    "    arcpy.Delete_management(fim_lyr)\n",
    "    arcpy.Delete_management(addr_lyr)\n",
    "\n",
    "    print(f\"✅  {output_name}_ADDR written to {output_gdb}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for shp in arcpy.ListFiles(\"*.shp\"):\n",
    "    full = os.path.join(folder, shp)\n",
    "\n",
    "    # 1-hour forecasts → two passes: ml_dis then wc_dis\n",
    "    if \"sr_nwm_tc_1hour\" in shp.lower():\n",
    "        out_name = \"FIM_1hour_ml\"\n",
    "        process_fim_flow(full, \"ml_dis\", out_name)\n",
    "        classify_lwc(os.path.join(output_gdb, out_name), out_name)\n",
    "        #classify_address_points(os.path.join(output_gdb, out_name), out_name)\n",
    "        out_name = \"FIM_1hour_wc\"\n",
    "        process_fim_flow(full, \"wc_dis\", out_name)\n",
    "        classify_lwc(os.path.join(output_gdb, out_name), out_name)\n",
    "        #classify_address_points(os.path.join(output_gdb, out_name), out_name)\n",
    "\n",
    "    # 2-hour forecasts → two passes: ml_dis then wc_dis\n",
    "    elif \"sr_nwm_tc_2hour\" in shp.lower():\n",
    "        out_name = \"FIM_2hour_ml\"\n",
    "        process_fim_flow(full, \"ml_dis\", out_name)\n",
    "        classify_lwc(os.path.join(output_gdb, out_name), out_name)\n",
    "        #classify_address_points(os.path.join(output_gdb, out_name), out_name)\n",
    "        out_name = \"FIM_2hour_wc\"\n",
    "        process_fim_flow(full, \"wc_dis\", out_name)\n",
    "        classify_lwc(os.path.join(output_gdb, out_name), out_name)\n",
    "        #classify_address_points(os.path.join(output_gdb, out_name), out_name)\n",
    "\n",
    "    # max-10-hour → single pass on streamflow\n",
    "    elif \"sr_nwm_tc_max10hr\" in shp.lower():\n",
    "        out_name = \"FIM_10hour_wc\"\n",
    "        #process_fim_flow(full, \"streamflow\", \"FIM_max_10hour\")\n",
    "        process_fim_flow(full, \"streamflow\", out_name)\n",
    "        classify_lwc(os.path.join(output_gdb, out_name), out_name)\n",
    "        #classify_address_points(os.path.join(output_gdb, out_name), out_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flonavsafe] *",
   "language": "python",
   "name": "conda-env-flonavsafe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
