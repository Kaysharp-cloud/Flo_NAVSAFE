{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cc3567",
   "metadata": {},
   "source": [
    "# Download NWM SR Forecast Data for July 4 - 7, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7885c",
   "metadata": {},
   "source": [
    "**Author(s):** \n",
    "\n",
    "<ul style=\"line-height:1.5;\">\n",
    "<li>Nana Oye Djan <a href=\"mailto:ndjan@andrew.cmu.edu\">(ndjan@andrew.cmu.edu)</a></li>\n",
    "</ul>\n",
    "\n",
    "**Last Updated:** \n",
    "17th July 2025\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This notebook provides code to retrieve NOAA National Water Model Short Range data from Amazon Web Services (AWS) in netcdf format for Travis County, Texas for the July 4 - 7 catastrophic flood events.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook downloads an ensmeble of 10 forecasts for each forecast time, from a predefined time to 10 hours ahead, run at different initialization times. For the July 4 - 7 events, the max discharge occured between July 5 and 6. To capture this peak, our predefined time was 5 PM UTC.  It takes a csv file of COMIDs, a layer in a geodatase with information on the geometries of the reaches (COMIDs) then retrieves data from AWS for Travis County, merges with the geodatabase layer, and saves them as shapefiles.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This notebook uses data developed and published by NOAA on Amazon Web Services (AWS) as described in detail in <a href=\"https://registry.opendata.aws/noaa-nwm-pds/\">(this registry)</a></li> of open data entry. The National Water Model (NWM) is a water resources model that simulates and forecasts water budget variables, including snowpack, evapotranspiration, soil moisture and streamflow, over the entire continental United States (CONUS). It is operated by NOAA’s Office of Water Prediction. This bucket contains a four-week rollover of the Short Range Forecast model output and the corresponding forcing data for the model. The model is forced with meteorological data from the High Resolution Rapid Refresh (HRRR) and the Rapid Refresh (RAP) models. The Short Range Forecast configuration cycles hourly and produces hourly deterministic forecasts of streamflow and hydrologic states out to 18 hours.  It also uses information on Travis County flowlines provided <a href=\"http://www.hydroshare.org/resource/c95e654312204ce0b4d8e31e71cd4354\">(here)</a></li>\n",
    "\n",
    "**Software Requirements:**\n",
    "\n",
    "This notebook uses Python v3.10.14 and requires the following specific Python libraries: \n",
    "\n",
    "> xarray: 2025.4.0     \n",
    "   geopandas: 0.14.4  \n",
    "   pandas: 2.3.0 \n",
    "   s3fs: 2025.5.1  \n",
    "   fsspec: 2025.5.1 \\\n",
    "   os: Python 3.10.14 (stdlib) \\\n",
    "   datetime / timedelta: Python 3.10.14 (stdlib) \\\n",
    "   re: Python 3.10.14 (stdlib)   \n",
    "\n",
    "**Disclosure**\n",
    "The code contained in this notebook was partially created and revised by ChatGPT, an AI language model developed by OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa5407",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import fsspec\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d1b66",
   "metadata": {},
   "source": [
    "### 2. Define important parameters and import necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77faf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 910 flowlines matching COMIDs.\n",
      "Found cycles [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]Z on 2025-07-05\n",
      "✔ Using t17Z as latest run\n"
     ]
    }
   ],
   "source": [
    "# (change only these two lines if you pick another day/hour)\n",
    "target_date          = datetime(2025, 7, 5).date()  # yyyy-mm-dd\n",
    "latest_run_hr_fixed  = 17                           # “now” cycle (UTC)\n",
    "\n",
    "#Import necessary files/Define necessary parameters\n",
    "comid_csv      = \"/Users/nanaoye/Documents/ArcGIS/Projects/Theme4DataRevised/Travis_Feature_IDs.csv\"\n",
    "flowlines_gpkg = \"/Users/nanaoye/Documents/ArcGIS/Projects/Theme4DataRevised/Theme4Data.gdb\"\n",
    "flowline_layer = \"P2FFlowlines\"\n",
    "\n",
    "comids        = pd.read_csv(comid_csv, dtype={\"IDs\": str})[\"IDs\"].tolist()\n",
    "flows         = gpd.read_file(flowlines_gpkg, layer=flowline_layer)\n",
    "flows         = flows.rename(columns={\"IDs\":\"feature_id\"})\n",
    "flows[\"feature_id\"] = flows[\"feature_id\"].astype(str)\n",
    "comids_int    = [int(c) for c in comids]\n",
    "\n",
    "print(f\"Loaded {len(flows)} flowlines matching COMIDs.\")\n",
    "\n",
    "out_dir = \"/Users/nanaoye/Library/CloudStorage/Box-Box/My Research/CUAHSI/SI_2025/sr_nwm_forecasts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#S3 bucket info\n",
    "s3_bucket     = \"noaa-nwm-pds\"\n",
    "forecast_path = \"short_range\"\n",
    "variable      = \"streamflow\"\n",
    "filename      = \"channel_rt\"\n",
    "max_lead_hours= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1484f",
   "metadata": {},
   "source": [
    "### 3. Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL builder\n",
    "def construct_s3_url(cycle_dt: datetime, lead_hr: int) -> str:\n",
    "    date_str = cycle_dt.strftime(\"%Y%m%d\")\n",
    "    cycle_hr = f\"{cycle_dt.hour:02d}\"\n",
    "    lead_str = f\"{lead_hr:03d}\"\n",
    "    fname = f\"nwm.t{cycle_hr}z.short_range.{filename}.f{lead_str}.conus.nc\"\n",
    "    return f\"s3://{s3_bucket}/nwm.{date_str}/{forecast_path}/{fname}\"\n",
    "\n",
    "#Define function to calculate worst case discharge and most likely discharge\n",
    "def collapse_with_runs(df):\n",
    "    df = df.copy().rename(columns={\"forecast_r\":\"forecast_run\",\n",
    "                                   \"forecast_t\":\"forecast_time\"})\n",
    "    df[\"forecast_r_dt\"] = pd.to_datetime(df[\"forecast_run\"])\n",
    "    df[\"forecast_t_dt\"] = pd.to_datetime(df[\"forecast_time\"])\n",
    "    out = []\n",
    "    for fid, grp in df.groupby(\"feature_id\", sort=False):\n",
    "        valid = grp[grp[\"forecast_r_dt\"] <= grp[\"forecast_t_dt\"]]\n",
    "        valid[\"lead_sec\"] = (valid[\"forecast_t_dt\"]-valid[\"forecast_r_dt\"]).dt.total_seconds()\n",
    "        top2 = valid.nlargest(2, \"lead_sec\")\n",
    "        if not top2.empty:\n",
    "            ml_row = top2.loc[top2[\"streamflow\"].idxmax()]\n",
    "            ml_dis, ml_r, ml_t = ml_row[\"streamflow\"], ml_row[\"forecast_run\"], ml_row[\"forecast_time\"]\n",
    "        else:\n",
    "            ml_dis = ml_r = ml_t = None\n",
    "        wc_row = grp.loc[grp[\"streamflow\"].idxmax()]\n",
    "        out.append({\"feature_id\":fid,\n",
    "                    \"ml_dis\":ml_dis,\"ml_run\":ml_r,\"ml_time\":ml_t,\n",
    "                    \"wc_dis\":wc_row[\"streamflow\"],\n",
    "                    \"wc_run\":wc_row[\"forecast_run\"],\n",
    "                    \"wc_time\":wc_row[\"forecast_time\"]})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Function to turn any datetime64 columns into text (Shapefile-safe)\n",
    "def scrub_datetimes(df, fmt=\"%Y-%m-%d %H:%M:%S\"):\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
    "            df[c] = df[c].dt.strftime(fmt)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29db296",
   "metadata": {},
   "source": [
    "### 4. Download SR Forecast for target date: July 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d83e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize file system                                \n",
    "fs      = s3fs.S3FileSystem(anon=True, client_kwargs={\"region_name\":\"us-east-1\"})\n",
    "prefix  = f\"{s3_bucket}/nwm.{target_date:%Y%m%d}/short_range/\"\n",
    "channel_files = [f for f in fs.find(prefix)\n",
    "                 if f.endswith(\".conus.nc\") and \".channel_rt.\" in f]\n",
    "\n",
    "if not channel_files:\n",
    "    raise RuntimeError(\"No channel_rt files for selected date\")\n",
    "\n",
    "published_runs = {int(re.search(r\"nwm\\.t(\\d{2})z\", os.path.basename(p)).group(1))\n",
    "                  for p in channel_files}\n",
    "print(f\"Found cycles {sorted(published_runs)}Z on {target_date}\")\n",
    "\n",
    "run_lead_map = {}\n",
    "for p in channel_files:\n",
    "    m_run  = re.search(r\"nwm\\.t(\\d{2})z\", os.path.basename(p))\n",
    "    m_lead = re.search(r\"\\.f(\\d{3})\\.\",  os.path.basename(p))\n",
    "    if m_run and m_lead:\n",
    "        run_hr, lead_hr = int(m_run.group(1)), int(m_lead.group(1))\n",
    "        if 1 <= lead_hr <= max_lead_hours:\n",
    "            run_lead_map[(run_hr, lead_hr)] = p\n",
    "\n",
    "prefix_dt      = datetime.combine(target_date, datetime.min.time())\n",
    "latest_run_hr  = latest_run_hr_fixed if latest_run_hr_fixed in published_runs else max(published_runs)\n",
    "latest_run_dt  = prefix_dt + timedelta(hours=latest_run_hr)\n",
    "\n",
    "# Determine latest initialization (forecast run) time                   \n",
    "records_current = []\n",
    "for lead_hr in range(1, max_lead_hours+1):\n",
    "    key = run_lead_map.get((latest_run_hr, lead_hr))\n",
    "    if not key: continue\n",
    "    with fs.open(key,\"rb\") as f:\n",
    "        ds = xr.open_dataset(f, engine=\"h5netcdf\")\n",
    "        ids = ds[\"feature_id\"].values.astype(int)\n",
    "        present = [cid for cid in comids_int if cid in ids]\n",
    "        if not present: continue\n",
    "        da = ds[variable].sel(feature_id=present).load()\n",
    "    df = da.to_dataframe().reset_index()\n",
    "    df[\"feature_id\"] = df[\"feature_id\"].astype(str)\n",
    "    df = df.set_index(\"feature_id\").reindex(comids).reset_index()\n",
    "    df[\"valid_time\"]   = latest_run_dt + timedelta(hours=lead_hr)\n",
    "    df[\"forecast_run\"] = latest_run_dt\n",
    "    df[\"lead_hour\"]    = lead_hr\n",
    "    records_current.append(df)\n",
    "\n",
    "df_current = pd.concat(records_current, ignore_index=True)\n",
    "print(f\"✔ Using t{latest_run_hr:02d}Z as latest run\")\n",
    "\n",
    "# Obtain Full ensemble for every valid hour on July 5 - 6 \n",
    "records_ensemble = []\n",
    "valid_times = [prefix_dt + timedelta(hours=h) for h in range(24)]  # 00–23 UTC\n",
    "\n",
    "for vt in valid_times:\n",
    "    for lead_hr in range(1, max_lead_hours+1):\n",
    "        run_dt = vt - timedelta(hours=lead_hr)\n",
    "        key    = run_lead_map.get((run_dt.hour, lead_hr))\n",
    "        if not key: continue\n",
    "        with fs.open(key,\"rb\") as f:\n",
    "            ds = xr.open_dataset(f, engine=\"h5netcdf\")\n",
    "            ids = ds[\"feature_id\"].values.astype(int)\n",
    "            pres= [cid for cid in comids_int if cid in ids]\n",
    "            if not pres: continue\n",
    "            da = ds[variable].sel(feature_id=pres).load()\n",
    "        dfe = da.to_dataframe().reset_index()\n",
    "        dfe[\"feature_id\"]   = dfe[\"feature_id\"].astype(str)\n",
    "        dfe = dfe.set_index(\"feature_id\").reindex(comids).reset_index()\n",
    "        dfe[\"forecast_time\"]= vt\n",
    "        dfe[\"forecast_run\"] = run_dt\n",
    "        dfe[\"lead_hour\"]    = lead_hr\n",
    "        records_ensemble.append(dfe)\n",
    "\n",
    "df_all = pd.concat(records_ensemble, ignore_index=True)\n",
    "df_all = df_all.rename(columns={\"forecast_time\":\"forecast_time\"})\n",
    "df_all['forecast_run']  = pd.to_datetime(df_all['forecast_run'])\n",
    "df_all['forecast_time'] = pd.to_datetime(df_all['forecast_time'])\n",
    "\n",
    "# optional: restrict forecast times to only times in target date.\n",
    "df_all = df_all[df_all[\"forecast_time\"].dt.date == target_date].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59dba0",
   "metadata": {},
   "source": [
    "### 5. Download shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7abc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished: 1-h, 2-h, max-10 h, and daily-peak shapefiles written\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1-h\n",
    "next_hour = latest_run_dt + timedelta(hours=1)\n",
    "df1_multi = df_all.query(\"forecast_time == @next_hour and lead_hour >= 1\")[[\n",
    "                \"feature_id\",\"forecast_time\",\"forecast_run\",\"streamflow\"]]\n",
    "df1_multi[\"streamflow\"] = df1_multi[\"streamflow\"].fillna(0)\n",
    "collapse_1h = collapse_with_runs(df1_multi.rename(columns={\"forecast_time\":\"forecast_t\",\n",
    "                                                           \"forecast_run\":\"forecast_r\"}))\n",
    "for df in (flows, collapse_1h):\n",
    "    df[\"feature_id\"] = pd.to_numeric(df[\"feature_id\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "gdf1_multi = (\n",
    "    flows[[\"LakeID\",\"HydroID\",\"From_Node\",\"To_Node\",\n",
    "             \"NextDownID\",\"feature_id\",\"order_\",\"areasqkm\",\"geometry\"]]\n",
    "      .merge(collapse_1h,on=\"feature_id\",how=\"left\")\n",
    ")\n",
    "gdf1_multi = scrub_datetimes(gdf1_multi)\n",
    "gdf1_multi.to_file(os.path.join(out_dir,\"sr_nwm_tc_1hour.shp\"))\n",
    "\n",
    "# 2-h\n",
    "target_time = latest_run_dt + timedelta(hours=2)\n",
    "df2_multi = df_all.query(\"forecast_time == @target_time\")[[\n",
    "                \"feature_id\",\"forecast_time\",\"forecast_run\",\"streamflow\"]]\n",
    "df2_multi[\"streamflow\"] = df2_multi[\"streamflow\"].fillna(0)\n",
    "collapse_2h = collapse_with_runs(df2_multi.rename(columns={\"forecast_time\":\"forecast_t\",\n",
    "                                                           \"forecast_run\":\"forecast_r\"}))\n",
    "# after you build collapse_2h but *before* the merge — add this line\n",
    "collapse_2h[\"feature_id\"] = pd.to_numeric(collapse_2h[\"feature_id\"],\n",
    "                                          errors=\"coerce\").astype(\"int\")\n",
    "keep_cols = [\"LakeID\",\"HydroID\",\"From_Node\",\"To_Node\",\n",
    "             \"NextDownID\",\"feature_id\",\"order_\",\"areasqkm\",\"geometry\"]\n",
    "gdf2_multi = flows[keep_cols].merge(collapse_2h,on=\"feature_id\",how=\"left\")\n",
    "gdf2_multi = scrub_datetimes(gdf2_multi)\n",
    "gdf2_multi.to_file(os.path.join(out_dir,\"sr_nwm_tc_2hour.shp\"))\n",
    "\n",
    "# Max-10 h discharge (existing)                                 \n",
    "latest_run = df_current[\"forecast_run\"].max()\n",
    "df10 = (df_all.groupby(\"feature_id\")[\"streamflow\"]\n",
    "              .max()\n",
    "              .reset_index()\n",
    "              .rename(columns={\"streamflow\":\"streamflow\"}))\n",
    "df10[\"datetime\"] = latest_run.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# after you build collapse_2h but *before* the merge — add this line\n",
    "df10[\"feature_id\"] = pd.to_numeric(df10[\"feature_id\"],\n",
    "                                          errors=\"coerce\").astype(\"int\")\n",
    "gdf10 = flows[keep_cols].merge(df10,on=\"feature_id\",how=\"left\")\n",
    "gdf10[\"streamflow\"] = gdf10[\"streamflow\"].fillna(0)\n",
    "gdf10      = scrub_datetimes(gdf10)\n",
    "gdf10.to_file(os.path.join(out_dir,\"sr_nwm_tc_max10hr.shp\"))\n",
    "\n",
    "# Optional: download daily peak (between 00 - 23 UTC) on target date                                   \n",
    "peak_df = (df_all.groupby(\"feature_id\")[\"streamflow\"]\n",
    "                  .max()\n",
    "                  .reset_index()\n",
    "                  .rename(columns={\"streamflow\":\"peak_dis\"}))\n",
    "peak_df[\"date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "peak_df[\"feature_id\"] = pd.to_numeric(peak_df[\"feature_id\"],\n",
    "                                          errors=\"coerce\").astype(\"int\")\n",
    "gdf_peak = flows[keep_cols].merge(peak_df,on=\"feature_id\",how=\"left\")\n",
    "gdf_peak[\"peak_dis\"] = gdf_peak[\"peak_dis\"].fillna(0)\n",
    "gdf_peak   = scrub_datetimes(gdf_peak)\n",
    "gdf_peak.to_file(os.path.join(out_dir,\"sr_nwm_tc_20250704_peak.shp\"))\n",
    "\n",
    "print(\"✓ Finished: 1-h, 2-h, max-10 h, and daily-peak shapefiles written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d45ef",
   "metadata": {},
   "source": [
    "### 6. Create shapefiles for NWM Analysis Assimilation Data for July 4 - 7, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52aec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>q_cfs_da</th>\n",
       "      <th>q_cfs_no_da</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5671171</td>\n",
       "      <td>9474.442607</td>\n",
       "      <td>19330.954902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5671165</td>\n",
       "      <td>9474.442607</td>\n",
       "      <td>19330.954902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5671187</td>\n",
       "      <td>4503.178951</td>\n",
       "      <td>14442.636764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5671185</td>\n",
       "      <td>5485.485901</td>\n",
       "      <td>13432.715701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5671181</td>\n",
       "      <td>6417.461499</td>\n",
       "      <td>11400.223133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>1629525</td>\n",
       "      <td>138.300948</td>\n",
       "      <td>138.300948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>1629809</td>\n",
       "      <td>138.356851</td>\n",
       "      <td>138.356851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>1629805</td>\n",
       "      <td>23.226317</td>\n",
       "      <td>23.226317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>1629803</td>\n",
       "      <td>23.218592</td>\n",
       "      <td>23.218592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>1629801</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>0.009182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID     q_cfs_da   q_cfs_no_da\n",
       "0    5671171  9474.442607  19330.954902\n",
       "1    5671165  9474.442607  19330.954902\n",
       "2    5671187  4503.178951  14442.636764\n",
       "3    5671185  5485.485901  13432.715701\n",
       "4    5671181  6417.461499  11400.223133\n",
       "..       ...          ...           ...\n",
       "911  1629525   138.300948    138.300948\n",
       "912  1629809   138.356851    138.356851\n",
       "913  1629805    23.226317     23.226317\n",
       "914  1629803    23.218592     23.218592\n",
       "915  1629801     0.009182      0.009182\n",
       "\n",
       "[916 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in csv file comtaining peak streamflows for NWM Analysis Assimilation with data assimilation and no (USGS) data assimilation\n",
    "da_no_da = pd.read_csv('/Users/nanaoye/Library/CloudStorage/Box-Box/My Research/CUAHSI/SI_2025/nwm_model_da_allq_120sec_20250704_nwm.csv')\n",
    "da_no_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create shapefile for data assimilation only flows\n",
    "da_only = da_no_da[['ID', 'q_cfs_da']].copy()\n",
    "da_only.rename(columns = {'ID': 'feature_id'}, inplace=True)\n",
    "da_only['streamflow'] = da_only['q_cfs_da']/35.314666212661\n",
    "da_only\n",
    "keep_cols = [\"LakeID\",\"HydroID\",\"From_Node\",\"To_Node\",\n",
    "             \"NextDownID\",\"feature_id\",\"order_\",\"areasqkm\",\"geometry\"]\n",
    "da_gdf= flows[keep_cols].merge(da_only,on=\"feature_id\",how=\"left\")\n",
    "da_gdf\n",
    "da_gdf.to_file(os.path.join(out_dir,\"jul4_da_max.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcb1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/24vf7y9j0x75b0vr13pfwvf40000gn/T/ipykernel_55671/871627407.py:9: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  no_da_gdf.to_file(os.path.join(out_dir,\"jul4_no_da_max.shp\"))\n"
     ]
    }
   ],
   "source": [
    "#Create shapefile for no data assimilation only flows\n",
    "no_da_only = da_no_da[['ID', 'q_cfs_no_da']].copy()\n",
    "no_da_only.rename(columns = {'ID': 'feature_id'}, inplace=True)\n",
    "no_da_only['streamflow'] = no_da_only['q_cfs_no_da']/35.314666212661\n",
    "no_da_only\n",
    "keep_cols = [\"LakeID\",\"HydroID\",\"From_Node\",\"To_Node\",\n",
    "             \"NextDownID\",\"feature_id\",\"order_\",\"areasqkm\",\"geometry\"]\n",
    "no_da_gdf= flows[keep_cols].merge(no_da_only,on=\"feature_id\",how=\"left\")\n",
    "no_da_gdf\n",
    "no_da_gdf.to_file(os.path.join(out_dir,\"jul4_no_da_max.shp\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
