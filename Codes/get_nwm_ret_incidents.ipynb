{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2226679d",
   "metadata": {},
   "source": [
    "# Retrieving and Aggregating NWM Data for TCSO Incident Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2171c7",
   "metadata": {},
   "source": [
    "**Author(s):** \n",
    "\n",
    "<ul style=\"line-height:1.5;\">\n",
    "<li>Nana Oye Djan <a href=\"mailto:ndjan@andrew.cmu.edu\">(ndjan@andrew.cmu.edu)</a></li>\n",
    "</ul>\n",
    "\n",
    "**Last Updated:** \n",
    "17th July 2025\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This notebook provides code to retrieve NOAA National Water Model Retrospective data from Amazon Web Services (AWS) in zarr format.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook downloads the NWM retrospective data for target dates and specified reaches and merges this downloaded data with flowlines specified by the user for use in our <a href=\"https://github.com/Kaysharp-cloud/Flo_NAVSAFE\">(FIM generation code)</a></li> .\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This notebook uses data developed and published by NOAA on Amazon Web Services (AWS) as described in detail in this registry <a href=\"https://registry.opendata.aws/nwm-archive/\">(this registry)</a></li> of open data entry. The NOAA National Water Model Retrospective dataset contains input and output from multi-decade CONUS retrospective simulations. These simulations used meteorological input fields from meteorological retrospective datasets. The output frequency and fields available in this historical NWM dataset differ from those contained in the real-time operational NWM forecast model. Additionally, note that no streamflow or other data assimilation is performed within any of the NWM retrospective simulations. This notebook uses the Zarr format version of this data. This notebook takes a csv file specifying the dates where flooded roadways and high water occurred in Travis County. It also takes a csv file of COMIDs, a layer in a geodatase with information on the geometries of the reaches (COMIDs) then retrieves data from AWS for Travis County and saves it as shapefiles. Necessary data can be found <a href=\"https://www.hydroshare.org/resource/41b23520d92c4d6e8d7934f106e54fd3/\">(here)</a></li> \n",
    "\n",
    "**Software Requirements:**\n",
    "\n",
    "This notebook requires the following specific Python libraries: \n",
    "\n",
    "> pandas: 2.3.0 \\\n",
    "   geopandas: 0.14.4 \\\n",
    "   tqdm: 4.67.1 \\\n",
    "   xarray: 2025.4.0 \\\n",
    "   s3fs: 2025.5.1 \\\n",
    "   matplotlib: 3.10.0 \\\n",
    "   shapely: 2.1.1 \\\n",
    "   os: Python 3.10.14 (stdlib) \\\n",
    "   pathlib: Python 3.10.14 (stdlib)  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50558a63",
   "metadata": {},
   "source": [
    "### 1. Install and Import Python Libraries Needed to Run this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "600ecfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662baad",
   "metadata": {},
   "source": [
    "### 2. Read In Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f48bef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read incident data from csv\n",
    "incidents_csv = Path('/Users/nanaoye/Library/CloudStorage/Box-Box/My Research/CUAHSI/SI_2025/all_flooded_incidents.csv')\n",
    "incidents_df  = pd.read_csv(incidents_csv, parse_dates=[\"response_date\"])\n",
    "target_dates = (incidents_df[\"response_date\"].dt.normalize().drop_duplicates().dt.strftime(\"%Y-%m-%d\").tolist())\n",
    "\n",
    "#Read list of TC COMIDs\n",
    "comid_csv = '/Users/nanaoye/Documents/ArcGIS/Projects/Theme4DataRevised/Travis_Feature_IDs.csv'\n",
    "comids = (pd.read_csv(comid_csv, dtype={\"IDs\": str})[\"IDs\"].astype(int).unique()).tolist()\n",
    "\n",
    "\n",
    "#Read flowlines\n",
    "flowlines_gpkg = '/Users/nanaoye/Documents/ArcGIS/Projects/Theme4DataRevised/Theme4Data.gdb'\n",
    "flowline_layer = 'P2FFlowlines'\n",
    "gdf_lines = gpd.read_file(flowlines_gpkg, layer=flowline_layer)\n",
    "gdf_lines[\"feature_id\"] = gdf_lines[\"feature_id\"].astype(int)\n",
    "\n",
    "#Specify output folder\n",
    "out_dir         = Path(\"/Users/nanaoye/Library/CloudStorage/Box-Box/My Research/CUAHSI/SI_2025/tcso_incidents_shps\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Variable read out of NWM Bucket\n",
    "variable = \"streamflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b10bcd",
   "metadata": {},
   "source": [
    "### 3. Get Retrospective Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3f2041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anonymous S3 filesystem object (public bucket)\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Point to the Zarr store\n",
    "mapper = fs.get_mapper('noaa-nwm-retrospective-3-0-pds/CONUS/zarr/chrtout.zarr')\n",
    "\n",
    "# Open the Zarr dataset using xarray\n",
    "ds = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "# Subset to TC\n",
    "ds_small = ds.sel(feature_id=comids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35051b6e",
   "metadata": {},
   "source": [
    "### Download shapefiles for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b901cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2015-03-09 → 2,245 reaches saved → nwm_peak_20150309.shp\n",
      "✅ 2015-03-21 → 2,245 reaches saved → nwm_peak_20150321.shp\n",
      "✅ 2015-05-05 → 2,245 reaches saved → nwm_peak_20150505.shp\n",
      "✅ 2015-05-06 → 2,245 reaches saved → nwm_peak_20150506.shp\n",
      "✅ 2015-05-13 → 2,245 reaches saved → nwm_peak_20150513.shp\n",
      "✅ 2015-05-17 → 2,245 reaches saved → nwm_peak_20150517.shp\n",
      "✅ 2015-05-23 → 2,245 reaches saved → nwm_peak_20150523.shp\n",
      "✅ 2015-05-24 → 2,245 reaches saved → nwm_peak_20150524.shp\n",
      "✅ 2015-05-29 → 2,245 reaches saved → nwm_peak_20150529.shp\n",
      "✅ 2015-06-18 → 2,245 reaches saved → nwm_peak_20150618.shp\n",
      "✅ 2015-06-27 → 2,245 reaches saved → nwm_peak_20150627.shp\n",
      "✅ 2015-10-24 → 2,245 reaches saved → nwm_peak_20151024.shp\n",
      "✅ 2015-10-26 → 2,245 reaches saved → nwm_peak_20151026.shp\n",
      "✅ 2015-10-30 → 2,245 reaches saved → nwm_peak_20151030.shp\n",
      "✅ 2015-10-31 → 2,245 reaches saved → nwm_peak_20151031.shp\n",
      "✅ 2015-11-28 → 2,245 reaches saved → nwm_peak_20151128.shp\n",
      "✅ 2015-12-13 → 2,245 reaches saved → nwm_peak_20151213.shp\n",
      "✅ 2016-03-09 → 2,245 reaches saved → nwm_peak_20160309.shp\n",
      "✅ 2016-05-14 → 2,245 reaches saved → nwm_peak_20160514.shp\n",
      "✅ 2016-05-19 → 2,245 reaches saved → nwm_peak_20160519.shp\n",
      "✅ 2016-05-26 → 2,245 reaches saved → nwm_peak_20160526.shp\n",
      "✅ 2016-05-27 → 2,245 reaches saved → nwm_peak_20160527.shp\n",
      "✅ 2016-05-30 → 2,245 reaches saved → nwm_peak_20160530.shp\n",
      "✅ 2016-05-31 → 2,245 reaches saved → nwm_peak_20160531.shp\n",
      "✅ 2016-06-03 → 2,245 reaches saved → nwm_peak_20160603.shp\n",
      "✅ 2016-08-17 → 2,245 reaches saved → nwm_peak_20160817.shp\n",
      "✅ 2016-08-20 → 2,245 reaches saved → nwm_peak_20160820.shp\n",
      "✅ 2016-08-21 → 2,245 reaches saved → nwm_peak_20160821.shp\n",
      "✅ 2022-02-03 → 2,245 reaches saved → nwm_peak_20220203.shp\n",
      "✅ 2022-03-22 → 2,245 reaches saved → nwm_peak_20220322.shp\n",
      "❌ Error on 2023-04-20: \"not all values found in index 'time'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "❌ Error on 2023-05-13: \"not all values found in index 'time'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "❌ Error on 2024-01-22: \"not all values found in index 'time'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "❌ Error on 2024-06-19: \"not all values found in index 'time'. Try setting the `method` keyword argument (example: method='nearest').\"\n"
     ]
    }
   ],
   "source": [
    "for date_str in target_dates:\n",
    "    try:\n",
    "        # Slice the COMID-subsetted dataset for the current day\n",
    "        ds_day = ds_small.sel(time=date_str)               # <<< CHANGED/ADDED\n",
    "\n",
    "        # Lazy → in-memory as a tidy DataFrame\n",
    "        df_day = (\n",
    "            ds_day[\"streamflow\"]\n",
    "              .to_dataframe()\n",
    "              .reset_index()\n",
    "              .dropna(subset=[\"streamflow\"])\n",
    "        )\n",
    "\n",
    "        if df_day.empty:\n",
    "            print(f\"⚠️ No NWM data on {date_str}\")\n",
    "            continue\n",
    "\n",
    "        # Extract the peak row for every reach\n",
    "        idxpeak  = df_day.groupby(\"feature_id\")[\"streamflow\"].idxmax()\n",
    "        df_peak  = df_day.loc[idxpeak].copy()              # <<< CHANGED/ADDED\n",
    "\n",
    "        # Trim/rename long columns\n",
    "        df_peak = df_peak.rename(\n",
    "            columns={\n",
    "                \"streamflow\": \"peak_cms\",                  # ≤10 chars\n",
    "                \"time\":       \"peak_time\"                  # ≤10 chars\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Ensure feature_id is int\n",
    "        df_peak[\"feature_id\"] = df_peak[\"feature_id\"].astype(int)\n",
    "\n",
    "        #merge onto your flowlines, keeping exactly the attributes you want:\n",
    "        keep = [\n",
    "            \"LakeID\", \"HydroID\", \"From_Node\", \"To_Node\",\n",
    "            \"NextDownID\", \"feature_id\", \"order_\", \"areasqkm\", \"geometry\"\n",
    "        ]\n",
    "\n",
    "        # Merge attributes onto flowline geometries\n",
    "        gdf_out = (\n",
    "            gdf_lines[keep].merge(df_peak, on=\"feature_id\", how=\"inner\")\n",
    "        )\n",
    "\n",
    "        #fill any missing reaches with zero\n",
    "        gdf_out[\"peak_cms\"] = gdf_out[\"peak_cms\"].fillna(0)\n",
    "\n",
    "        \n",
    "        if gdf_out.empty:\n",
    "            print(f\"⚠️ Join produced zero rows on {date_str}\")\n",
    "            continue\n",
    "\n",
    "        #Drop peak time\n",
    "        gdf_out = gdf_out.drop(columns=[\"peak_time\"]) \n",
    "\n",
    "        # Fix object-type columns (especially for SHP export)\n",
    "        for col in gdf_out.columns:\n",
    "            if gdf_out[col].dtype == object:\n",
    "                gdf_out[col] = gdf_out[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "\n",
    "        # Write Shapefile (will inherit flowline CRS)\n",
    "        shp_name = out_dir / f\"nwm_peak_{date_str.replace('-','')}.shp\"\n",
    "        gdf_out.to_file(shp_name)\n",
    "        print(\n",
    "            f\"✅ {date_str} → {len(gdf_out):,} reaches saved → {shp_name.name}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error on {date_str}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
